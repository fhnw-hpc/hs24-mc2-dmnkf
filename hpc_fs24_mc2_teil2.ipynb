{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Mini-Challenge 2 - Beschleunigung in Data Science\n",
    "## Teil 2: GPU\n",
    "#### FHNW - FS2024\n",
    "\n",
    "Original von S. Suter, angepasst von S. Marcin und M. Stutz\n",
    "\n",
    "Abgabe von: <font color='blue'>Dominik Filliger</font>\n",
    "\n",
    "#### Ressourcen\n",
    "* [Überblick GPU Programmierung](https://www.cherryservers.com/blog/introduction-to-gpu-programming-with-cuda-and-python)\n",
    "* [CUDA Basic Parts](https://nyu-cds.github.io/python-gpu/02-cuda/)\n",
    "* [Accelerate Code with CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56)\n",
    "* Vorlesungen und Beispiele aus dem Informatikkurs PAC (parallel computing), siehe Ordner \"resources\"\n",
    "* CSCS \"High-Performance Computing with Python\" Kurs, Tag 3: \n",
    "    - JIT Numba GPU 1 + 2\n",
    "    - https://youtu.be/E4REVbCVxNQ\n",
    "    - https://github.com/eth-cscs/PythonHPC/tree/master/numba-cuda\n",
    "    - Siehe auch aktuelles Tutorial von 2021\n",
    "* [Google CoLab](https://colab.research.google.com/) oder ggf. eigene GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "import logging\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "from numba import cuda, vectorize, float32\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO REMOVE FOR FINAL SUBMISSION\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_mri_images(subfolder='001'):\n",
    "    \"\"\"Load MRI images from the specified subfolder\"\"\"\n",
    "    try:\n",
    "        folders = os.path.join('adni_png', subfolder)\n",
    "        if not os.path.exists(folders):\n",
    "            raise FileNotFoundError(f\"MRI image folder not found: {folders}\")\n",
    "        \n",
    "        files = sorted(glob.glob(f\"{folders}/*.png\"))\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No MRI images found in {folders}\")\n",
    "        \n",
    "        images = np.array([imageio.imread(f) for f in files], dtype=np.float32)\n",
    "        names = [f[-17:-4] for f in files]\n",
    "        logger.info(f\"Successfully loaded {len(images)} MRI images from {subfolder}\")\n",
    "        return images, names\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading MRI images: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/miniconda3/envs/hpc/lib/python3.11/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n",
       "       63.992188 ], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy Beispiel zum testen mit Numba\n",
    "\n",
    "import math\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def gpu_sqrt(x):\n",
    "    return math.sqrt(x)\n",
    "  \n",
    "\n",
    "a = np.arange(4096,dtype=np.float32)\n",
    "gpu_sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 GPU Rekonstruktion\n",
    "\n",
    "Implementiere eine SVD-Rekonstruktionsvariante auf der GPU oder in einem hybriden Setting. Code aus dem ersten Teil darf dabei verwendet werden. Wähle  bewusst, welche Teile des Algorithms in einem GPU Kernel implementiert werden und welche effizienter auf der CPU sind. Ziehe dafür Erkenntnisse aus dem ersten Teil mit ein. Es muss mindestens eine Komponente des Algorithmuses in einem GPU-Kernel implementiert werden. Dokumentiere Annahmen, welche du ggf. zur Vereinfachung triffst. Evaluiere, ob du mit CuPy oder Numba arbeiten möchtest.\n",
    "\n",
    "Links:\n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from numba import cuda\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "# todo 1\n",
    "def reconstruct_svd_broadcast1(u,s,vt,k):\n",
    "    ### BEGIN SOLUTION\n",
    "    reco = u[:,:k] * s[:k] @ vt[:k,:]\n",
    "    ### END SOLUTION\n",
    "    return reco\n",
    "\n",
    "def reconstruct_svd_cp(u, s, vt, k):\n",
    "    \"\"\"\n",
    "    Perform SVD reconstruction using CuPy's built-in dot product.\n",
    "    \"\"\"\n",
    "    return cp.asnumpy(cp.dot(u[:,:k], cp.dot(cp.diag(s[:k]), vt[:k,:])))\n",
    "\n",
    "def reconstruct_svd_cp_einsum(u, s, vt, k):\n",
    "    \"\"\"\n",
    "    Perform SVD reconstruction using CuPy and einsum for matrix multiplication.\n",
    "    \"\"\"\n",
    "    return cp.asnumpy(cp.einsum('ik,k,kj->ij', u[:,:k], s[:k], vt[:k,:]))\n",
    "\n",
    "def reconstruct_svd_cp_broadcast(u, s, vt, k):\n",
    "    \"\"\"\n",
    "    CuPy SVD reconstruction using broadcasting for the multiplication of S.\n",
    "    \"\"\"\n",
    "    return cp.asnumpy(cp.dot(u[:,:k], cp.multiply(s[:k].reshape(-1, 1), vt[:k,:])))\n",
    "\n",
    "@cuda.jit\n",
    "def reconstruct_svd_kernel(u, s, vt, out, k):\n",
    "    \"\"\"\n",
    "    CUDA kernel that reconstructs a matrix from SVD components:\n",
    "      out = u * s * vt, up to rank k.\n",
    "    \"\"\"\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < u.shape[0] and j < vt.shape[1]:\n",
    "        tmp = 0.0\n",
    "        for r in range(k):\n",
    "            tmp += u[i, r] * s[r] * vt[r, j]\n",
    "        out[i, j] = tmp\n",
    "\n",
    "def reconstruct_svd_numba(u, s, vt, k):\n",
    "    \"\"\"\n",
    "    Multiply U, S, and V^T on the GPU using a custom CUDA kernel.\n",
    "    \"\"\"\n",
    "    U_k = u[:,:k]\n",
    "    S_k = s[:k]\n",
    "    VT_k = vt[:k,:]\n",
    "\n",
    "    # Prepare output on GPU\n",
    "    out = np.zeros((U_k.shape[0], VT_k.shape[1]), dtype=np.float32)\n",
    "\n",
    "    # Launch kernel\n",
    "    threads_per_block = (16, 16)\n",
    "    blocks_per_grid = (\n",
    "        math.ceil(U_k.shape[0] / threads_per_block[0]),\n",
    "        math.ceil(VT_k.shape[1] / threads_per_block[1])\n",
    "    )\n",
    "    reconstruct_svd_kernel[blocks_per_grid, threads_per_block](U_k, S_k, VT_k, out, S_k.shape[0])\n",
    "    return out\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def measure_metrics(original, reconstructed):\n",
    "    \"\"\"\n",
    "    Compute MSE and PSNR for the given original and reconstructed images.\n",
    "    \"\"\"\n",
    "    mse = np.mean((original - reconstructed) ** 2)\n",
    "    if mse == 0:  # Perfect reconstruction\n",
    "        psnr = float('inf')\n",
    "    else:\n",
    "        psnr = round(20 * np.log10(255.0 / np.sqrt(mse)), 5)\n",
    "    return mse, psnr\n",
    "\n",
    "def plot_reconstruction_comparison(test_image, k_values=(10, 25, 50, 100)):\n",
    "    \"\"\"\n",
    "    For each method, reconstruct the test_image with various k-values,\n",
    "    measuring decomposition and reconstruction times separately.\n",
    "    \"\"\"\n",
    "    implementations = {\n",
    "        'CuPy Basic': reconstruct_svd_cp,\n",
    "        'CuPy Einsum': reconstruct_svd_cp_einsum,\n",
    "        'CuPy Broadcast': reconstruct_svd_cp_broadcast,\n",
    "        'Numba CUDA': reconstruct_svd_numba,\n",
    "        'CPU Broadcast': reconstruct_svd_broadcast1,\n",
    "    }\n",
    "\n",
    "    results = {name: [] for name in implementations}\n",
    "\n",
    "    # Perform SVD decomposition once\n",
    "    U, S, VT = np.linalg.svd(test_image, full_matrices=False)\n",
    "\n",
    "    # Pre-transfer data to GPU for GPU implementations\n",
    "    U_gpu = cp.asarray(U, dtype=cp.float32)\n",
    "    S_gpu = cp.asarray(S, dtype=cp.float32)\n",
    "    VT_gpu = cp.asarray(VT, dtype=cp.float32)\n",
    "\n",
    "    for k in k_values:\n",
    "        first_mse = None\n",
    "        first_psnr = None\n",
    "        \n",
    "        for name, func in implementations.items():\n",
    "            recon_times, mse_list, psnr_list = [], [], []\n",
    "            \n",
    "            # Prepare inputs based on implementation type\n",
    "            if name == 'CPU Broadcast':\n",
    "                u_input, s_input, vt_input = U, S, VT\n",
    "            else:\n",
    "                u_input, s_input, vt_input = U_gpu, S_gpu, VT_gpu\n",
    "            # warmup\n",
    "            func(u_input, s_input, vt_input, k)\n",
    "\n",
    "            recon_time = timeit.timeit(\n",
    "                lambda: func(u_input, s_input, vt_input, k),\n",
    "                number=3\n",
    "            )\n",
    "            reconstructed = func(u_input, s_input, vt_input, k)\n",
    "            \n",
    "            mse, psnr = measure_metrics(test_image, reconstructed)\n",
    "            \n",
    "            # Store first MSE/PSNR values to compare against\n",
    "            if first_mse is None:\n",
    "                first_mse = mse\n",
    "                first_psnr = psnr\n",
    "            else:\n",
    "                # Assert that MSE and PSNR are equal across implementations\n",
    "                assert np.allclose(mse, first_mse, rtol=1e-5), f\"MSE mismatch for {name}\"\n",
    "                assert np.allclose(psnr, first_psnr, rtol=1e-5), f\"PSNR mismatch for {name}\"\n",
    "\n",
    "            results[name].append({\n",
    "                'k': k,\n",
    "                'recon_time': recon_time,\n",
    "                'mse': mse,\n",
    "                'psnr': psnr,\n",
    "                'reconstructed': reconstructed\n",
    "            })\n",
    "\n",
    "    # Plot reconstruction time, MSE, PSNR\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    for name, data in results.items():\n",
    "        k_list = [d['k'] for d in data]\n",
    "        recon_times = [d['recon_time'] for d in data]\n",
    "        mse_list = [d['mse'] for d in data]\n",
    "        psnr_list = [d['psnr'] for d in data]\n",
    "\n",
    "        axes[0].plot(k_list, recon_times, '-o', label=name)\n",
    "        axes[1].plot(k_list, mse_list, '-o', label=name)\n",
    "        axes[2].plot(k_list, psnr_list, '-o', label=name)\n",
    "\n",
    "    axes[0].set_title('Reconstruction Time')\n",
    "    axes[0].set_xlabel('k')\n",
    "    axes[0].set_ylabel('Time (s)')\n",
    "    axes[0].set_yscale('log')\n",
    "\n",
    "    axes[1].set_title('MSE')\n",
    "    axes[1].set_xlabel('k')\n",
    "    axes[1].set_ylabel('Mean Squared Error')\n",
    "    # log scale\n",
    "    axes[1].set_yscale('log')\n",
    "\n",
    "    axes[2].set_title('PSNR')\n",
    "    axes[2].set_xlabel('k')\n",
    "    axes[2].set_ylabel('dB')\n",
    "    # log scale\n",
    "    axes[2].set_yscale('log')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot reconstructions and difference maps for k=50\n",
    "    k_idx = k_values.index(50)  # Get index of k=50 results\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    n_methods = len(implementations)\n",
    "    fig, axes = plt.subplots(3, n_methods, figsize=(4*n_methods, 12))\n",
    "    \n",
    "    # Plot original image in first row, first column\n",
    "    axes[0,0].imshow(test_image, cmap='gray')\n",
    "    axes[0,0].set_title('Original Image')\n",
    "    axes[0,0].axis('off')\n",
    "    \n",
    "    for idx, (name, method_results) in enumerate(results.items()):\n",
    "        reconstructed = method_results[k_idx]['reconstructed']\n",
    "        diff_map = np.abs(test_image - reconstructed)\n",
    "        \n",
    "        # Plot reconstructed image\n",
    "        if idx > 0:  # Skip first column of first row (used for original)\n",
    "            axes[0,idx].imshow(reconstructed, cmap='gray')\n",
    "            axes[0,idx].set_title(f'{name}\\nk=50')\n",
    "            axes[0,idx].axis('off')\n",
    "        \n",
    "        # Plot difference map\n",
    "        im = axes[1,idx].imshow(diff_map, cmap='hot')\n",
    "        axes[1,idx].set_title(f'Difference Map\\nMSE={method_results[k_idx][\"mse\"]:.2e}')\n",
    "        axes[1,idx].axis('off')\n",
    "        plt.colorbar(im, ax=axes[1,idx])\n",
    "        \n",
    "        # Plot intensity profile\n",
    "        middle_row = test_image.shape[0]//2\n",
    "        axes[2,idx].plot(test_image[middle_row,:], label='Original')\n",
    "        axes[2,idx].plot(reconstructed[middle_row,:], '--', label='Reconstructed')\n",
    "        axes[2,idx].set_title('Middle Row Intensity Profile')\n",
    "        axes[2,idx].legend()\n",
    "        axes[2,idx].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "images, names = load_mri_images(subfolder='001')\n",
    "test_image = np.ascontiguousarray(images[0])\n",
    "\n",
    "plot_reconstruction_comparison(test_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 GPU-Kernel Performance\n",
    "\n",
    "##### 5.2.1 Blocks und Input-Grösse\n",
    "\n",
    "Links: \n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n",
    "* [NVIDIA Kapitel zu \"Strided Access\"](https://spaces.technik.fhnw.ch/multimediathek/file/cuda-best-practices-in-c)\n",
    "* https://developer.nvidia.com/blog/cublas-strided-batched-matrix-multiply/\n",
    "* https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/\n",
    "\n",
    "Führe 2-3 Experimente mit unterschiedlichen Blockkonfigurationen und Grösse der Input-Daten durch. Erstelle dafür ein neues Datenset mit beliebig grossen Matrizen, da die GPU besonders geeignet ist um grosse Inputs zu verarbeiten (Verwende diese untschiedlich grossen Matrizen für alle nachfolgenden Vergeliche und Tasks ebenfalls). Messe die Performance des GPU-Kernels mittels geeigneten Funktionen. Welche Blockgrösse in Abhängigkeit mit der Input-Grösse hat sich bei dir basierend auf deinen Experimenten als am erfolgreichsten erwiesen? Welches sind deiner Meinung nach die Gründe dafür? Wie sind die Performance Unterschiede zwischen deiner CPU und GPU Implementierung? Diskutiere deine Analyse (ggf. mit Grafiken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix sizes to test\n",
    "matrix_sizes = [128, 512, 1024, 2048, 4096]\n",
    "\n",
    "# Block configurations to test - powers of 2, respecting max 1024 threads\n",
    "block_configs = [\n",
    "    (x, y) for x, y in [\n",
    "        (2**i, 2**j) \n",
    "        for i in range(1,6) \n",
    "        for j in range(1,9)\n",
    "    ] if x * y <= 1024 and x <= y\n",
    "]\n",
    "\n",
    "results_basic = []\n",
    "\n",
    "# Precompute SVD decompositions for each matrix size\n",
    "svd_components = {}\n",
    "for N in matrix_sizes:\n",
    "    A = np.random.randn(N, N).astype(np.float32)\n",
    "    u, s, vt = cp.linalg.svd(cp.asarray(A), full_matrices=False)\n",
    "    k = min(u.shape[1], vt.shape[0])\n",
    "    svd_components[N] = {\n",
    "        'u': cp.asnumpy(u),\n",
    "        's': cp.asnumpy(s),\n",
    "        'vt': cp.asnumpy(vt),\n",
    "        'k': k\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "for N in matrix_sizes:\n",
    "    # Get precomputed components\n",
    "    u = svd_components[N]['u']\n",
    "    s = svd_components[N]['s'] \n",
    "    vt = svd_components[N]['vt']\n",
    "    k = svd_components[N]['k']\n",
    "\n",
    "    # CPU baseline\n",
    "    start_cpu = time.time()\n",
    "    C_cpu = reconstruct_svd_broadcast1(u, s, vt, k)\n",
    "    cpu_time = time.time() - start_cpu\n",
    "    print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "    \n",
    "    # Allocate device memory\n",
    "    u_device = cuda.to_device(u)\n",
    "    s_device = cuda.to_device(s)\n",
    "    vt_device = cuda.to_device(vt)\n",
    "    C_device = cuda.device_array((N, N), dtype=np.float32)\n",
    "\n",
    "    for block_size in block_configs:\n",
    "        threadsperblock = block_size\n",
    "        blockspergrid_x = math.ceil(N / threadsperblock[0])\n",
    "        blockspergrid_y = math.ceil(N / threadsperblock[1])\n",
    "        blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "        \n",
    "        # Warm-up run\n",
    "        reconstruct_svd_kernel[blockspergrid, threadsperblock](u_device, s_device, vt_device, C_device, k)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Timed run\n",
    "        start_gpu = time.time()\n",
    "        reconstruct_svd_kernel[blockspergrid, threadsperblock](u_device, s_device, vt_device, C_device, k)\n",
    "        gpu_time = time.time() - start_gpu\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Copy result back to host\n",
    "        C_gpu = C_device.copy_to_host()\n",
    "        \n",
    "        # Calculate error\n",
    "        error = np.mean(np.abs(C_cpu - C_gpu))\n",
    "        speedup = cpu_time / gpu_time\n",
    "        \n",
    "        results_basic.append({\n",
    "            'kernel': 'basic',\n",
    "            'matrix_size': N,\n",
    "            'block_size': block_size,\n",
    "            'cpu_time': cpu_time,\n",
    "            'gpu_time': gpu_time,\n",
    "            'speedup': speedup,\n",
    "        })\n",
    "        \n",
    "        print(f\"Block size {block_size}: GPU time = {gpu_time:.4f}s, Speedup = {speedup:.2f}x, Error = {error:.2e}\")\n",
    "        \n",
    "    # Clean up device memory\n",
    "    del u_device, s_device, vt_device, C_device\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, kernel_name):\n",
    "    \"\"\"\n",
    "    Plot heatmaps and bar plots for each matrix size showing GPU times and CPU comparison.\n",
    "    \"\"\"\n",
    "    # Extract unique sizes and configurations\n",
    "    block_sizes = sorted(list(set([tuple(r['block_size']) for r in results])))\n",
    "    matrix_sizes = sorted(list(set(r['matrix_size'] for r in results)))\n",
    "    \n",
    "    # Prepare data structures\n",
    "    gpu_times = {}\n",
    "    cpu_times = {}\n",
    "    \n",
    "    # Organize data\n",
    "    for r in results:\n",
    "        bs = tuple(r['block_size'])\n",
    "        N = r['matrix_size']\n",
    "        gpu_times.setdefault(bs, {})[N] = r['gpu_time']\n",
    "        if N not in cpu_times:\n",
    "            cpu_times[N] = r['cpu_time']\n",
    "    \n",
    "    # Create subplots for each matrix size\n",
    "    for N in matrix_sizes:\n",
    "        plt.figure(figsize=(18, 5))\n",
    "        \n",
    "        # --- Left subplot: Heatmap\n",
    "        plt.subplot(1, 2, 1)\n",
    "        \n",
    "        # Extract block dimensions for heatmap axes\n",
    "        block_x = sorted(list(set([bs[0] for bs in block_sizes])))\n",
    "        block_y = sorted(list(set([bs[1] for bs in block_sizes])))\n",
    "        \n",
    "        # Create heatmap data - fill lower triangle\n",
    "        heatmap_data = np.zeros((len(block_x), len(block_y)))\n",
    "        for i, bx in enumerate(block_x):\n",
    "            for j, by in enumerate(block_y):\n",
    "                if (bx, by) in gpu_times and N in gpu_times[(bx, by)]:\n",
    "                    heatmap_data[i,j] = gpu_times[(bx, by)][N]\n",
    "                else:\n",
    "                    heatmap_data[i,j] = np.nan\n",
    "        \n",
    "        # Find minimum time (excluding NaN values)\n",
    "        min_time = np.nanmin(heatmap_data)\n",
    "        \n",
    "        plt.imshow(heatmap_data, cmap='magma_r', aspect='auto')  # Reversed colormap so lowest time is highlighted\n",
    "        plt.colorbar(label='Time (s)')\n",
    "        plt.title(f'GPU Times Heatmap - Matrix Size {N}x{N}\\nBest time: {min_time:.4f}s')\n",
    "        plt.xlabel('Block Y Size')\n",
    "        plt.ylabel('Block X Size')\n",
    "        plt.xticks(range(len(block_y)), block_y)\n",
    "        plt.yticks(range(len(block_x)), block_x)\n",
    "        \n",
    "        # Highlight the minimum value\n",
    "        min_idx = np.where(heatmap_data == min_time)\n",
    "        plt.plot(min_idx[1], min_idx[0], 'r*', markersize=15, label=f'Best: {min_time:.4f}s')\n",
    "        plt.legend()\n",
    "        \n",
    "        # --- Right subplot: Bar plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        \n",
    "        # Prepare data for bar plot\n",
    "        gpu_times_N = []\n",
    "        gpu_labels = []\n",
    "        \n",
    "        for bs in block_sizes:\n",
    "            if N in gpu_times[bs]:\n",
    "                gpu_times_N.append(gpu_times[bs][N])\n",
    "                gpu_labels.append(f'Block {bs}')\n",
    "        \n",
    "        # Sort by execution time\n",
    "        sorted_indices = np.argsort(gpu_times_N)\n",
    "        sorted_times = [gpu_times_N[i] for i in sorted_indices]\n",
    "        sorted_labels = [gpu_labels[i] for i in sorted_indices]\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = plt.bar(range(len(sorted_times)), sorted_times)\n",
    "        plt.xticks(range(len(sorted_times)), sorted_labels, rotation=45, ha='right')\n",
    "        \n",
    "        # Add CPU time reference line\n",
    "        plt.axhline(y=cpu_times[N], color='r', linestyle='--', label=f'CPU Time')\n",
    "        \n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Time (s)')\n",
    "        plt.title(f'Execution Times - Matrix Size {N}x{N}')\n",
    "        plt.grid(True, which='both', ls='-', alpha=0.2)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_basic, 'Basic Kernel')\n",
    "\n",
    "matrix_sizes = sorted(list(set(r['matrix_size'] for r in results_basic)))\n",
    "\n",
    "for N in matrix_sizes:\n",
    "    # Filter results for this matrix size\n",
    "    results_N = [r for r in results_basic if r['matrix_size'] == N]\n",
    "    best_basic = min(results_N, key=lambda x: x['gpu_time'])\n",
    "    \n",
    "    print(f\"\\n=== Best Configuration for Basic Kernel (Matrix Size {N}x{N}) ===\")\n",
    "    print(f\"Block Size: {best_basic['block_size']}\")\n",
    "    print(f\"GPU Time: {best_basic['gpu_time']:.4f} seconds\") \n",
    "    print(f\"Speedup over CPU: {best_basic['speedup']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Shared Memory auf der GPU\n",
    "Optimiere deine Implementierung von oben indem du das shared Memory der GPU verwendest. Führe wieder mehrere Experimente mit unterschiedlicher Datengrösse durch und evaluiere den Speedup gegenüber der CPU Implementierung.\n",
    "\n",
    "Links:\n",
    "* [Best Practices Memory Optimizations](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)\n",
    "* [Examples: Matrix Multiplikation und Shared Memory](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def get_kernel(threads_per_block):\n",
    "    @cuda.jit\n",
    "    def reconstruct_svd_numba_shared_memory(u, s, vt, C, k):\n",
    "        block_i = cuda.blockIdx.x\n",
    "        block_j = cuda.blockIdx.y\n",
    "        thread_i = cuda.threadIdx.x\n",
    "        thread_j = cuda.threadIdx.y\n",
    "        i, j = cuda.grid(2)\n",
    "\n",
    "        tmp = 0.0\n",
    "        \n",
    "        # generate shared memory arrays of size threads_per_block x threads_per_block\n",
    "        u_shared = cuda.shared.array(shape=(threads_per_block[0], threads_per_block[1]), dtype=float32)\n",
    "        vt_shared = cuda.shared.array(shape=(threads_per_block[0], threads_per_block[1]), dtype=float32)\n",
    "        s_shared = cuda.shared.array(shape=(threads_per_block[0]), dtype=float32)\n",
    "\n",
    "        # determine how many blocks we need to load\n",
    "        num_blocks = math.ceil(min(k, vt.shape[0], u.shape[1]) / threads_per_block[0])\n",
    "        for m in range(num_blocks):\n",
    "            u_shared[thread_i, thread_j] = u[block_i * threads_per_block[0] + thread_i, m * threads_per_block[1] + thread_j]\n",
    "            vt_shared[thread_i, thread_j] = vt[m * threads_per_block[0] + thread_i, block_j * threads_per_block[1] + thread_j]\n",
    "            if thread_j == 0:\n",
    "                s_shared[thread_i] = s[m * threads_per_block[0] + thread_i]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "            for l in range(threads_per_block[0]):\n",
    "                if l + m * threads_per_block[0] < k:\n",
    "                    tmp += u_shared[thread_i, l] * s_shared[l] * vt_shared[l, thread_j]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        C[i, j] = tmp\n",
    "        \n",
    "    return reconstruct_svd_numba_shared_memory\n",
    "\n",
    "\n",
    "\n",
    "# Store results\n",
    "results_shared = []\n",
    "\n",
    "for N in matrix_sizes:\n",
    "    # Get precomputed components\n",
    "    u = svd_components[N]['u']\n",
    "    s = svd_components[N]['s'] \n",
    "    vt = svd_components[N]['vt']\n",
    "    k = svd_components[N]['k']\n",
    "\n",
    "    # CPU baseline\n",
    "    start_cpu = time.time()\n",
    "    C_cpu = reconstruct_svd_broadcast1(u, s, vt, k)\n",
    "    cpu_time = time.time() - start_cpu\n",
    "    print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "    \n",
    "    # Allocate device memory\n",
    "    u_device = cuda.to_device(u)\n",
    "    s_device = cuda.to_device(s)\n",
    "    vt_device = cuda.to_device(vt)\n",
    "    C_device = cuda.device_array((N, N), dtype=np.float32)\n",
    "\n",
    "    for block_size in block_configs:\n",
    "        threadsperblock = block_size\n",
    "        blockspergrid_x = math.ceil(N / threadsperblock[0])\n",
    "        blockspergrid_y = math.ceil(N / threadsperblock[1])\n",
    "        blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "        \n",
    "        kernel = get_kernel(threadsperblock)\n",
    "        \n",
    "        # Warm-up run\n",
    "        kernel[blockspergrid, threadsperblock](u_device, s_device, vt_device, C_device, k)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Timed run\n",
    "        start_gpu = time.time()\n",
    "        kernel[blockspergrid, threadsperblock](u_device, s_device, vt_device, C_device, k)\n",
    "        gpu_time = time.time() - start_gpu\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Copy result back to host\n",
    "        C_gpu = C_device.copy_to_host()\n",
    "        \n",
    "        speedup = cpu_time / gpu_time\n",
    "        \n",
    "        results_shared.append({\n",
    "            'kernel': 'shared',\n",
    "            'matrix_size': N,\n",
    "            'block_size': block_size,\n",
    "            'cpu_time': cpu_time,\n",
    "            'gpu_time': gpu_time,\n",
    "            'speedup': speedup,\n",
    "        })\n",
    "        \n",
    "        print(f\"Block size {block_size}: GPU time = {gpu_time:.4f}s, Speedup = {speedup:.2f}x\")\n",
    "        \n",
    "    # Clean up device memory\n",
    "    del u_device, s_device, vt_device, C_device\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_shared, 'Shared Kernel')\n",
    "\n",
    "best_shared = min(results_shared, key=lambda x: x['gpu_time'])\n",
    "\n",
    "for N in matrix_sizes:\n",
    "    # Filter results for this matrix size\n",
    "    results_N = [r for r in results_shared if r['matrix_size'] == N]\n",
    "    best_shared = min(results_N, key=lambda x: x['gpu_time'])\n",
    "    \n",
    "    print(f\"\\n=== Best Configuration for Shared Kernel (Matrix Size {N}x{N}) ===\")\n",
    "    print(f\"Block Size: {best_shared['block_size']}\")\n",
    "    print(f\"GPU Time: {best_shared['gpu_time']:.4f} seconds\") \n",
    "    print(f\"Speedup over CPU: {best_shared['speedup']:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was sind deine Erkenntnisse bzgl. GPU-Memory-Allokation und des Daten-Transferes auf die GPU? Interpretiere deine Resultate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3 Bonus: Weitere Optimierungen\n",
    "Optimiere deine Implementation von oben weiter. Damit du Erfolg hast, muss der Data-Reuse noch grösser sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def allocate_pinned(shape, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Allocate pinned (page-locked) memory on the host.\n",
    "    This can help reduce transfer overhead to/from the GPU.\n",
    "    \"\"\"\n",
    "    return cuda.pinned_array(shape, dtype=dtype)\n",
    "\n",
    "def get_optimized_kernel(threads_per_block):\n",
    "    \"\"\"\n",
    "    Returns an 'optimized' GPU kernel for partial SVD reconstruction\n",
    "    that builds on the shared memory approach and uses a bit more\n",
    "    local register logic to reduce repeated memory loads.\n",
    "    \"\"\"\n",
    "    blockDimX, blockDimY = threads_per_block\n",
    "\n",
    "    @cuda.jit\n",
    "    def reconstruct_svd_optimized(u, s, vt, C, k):\n",
    "        \"\"\"\n",
    "        Further optimized reconstruction:\n",
    "          - We still chunk across k in increments of blockDimX\n",
    "          - We keep partial sums in registers\n",
    "          - We rely on shared memory for local tile data\n",
    "        \"\"\"\n",
    "        # If k <= 0, no reconstruction\n",
    "        if k <= 0:\n",
    "            return\n",
    "\n",
    "        # Global (i, j)\n",
    "        i = cuda.blockIdx.x * blockDimX + cuda.threadIdx.x\n",
    "        j = cuda.blockIdx.y * blockDimY + cuda.threadIdx.y\n",
    "\n",
    "        # Boundary checks\n",
    "        if i >= C.shape[0] or j >= C.shape[1]:\n",
    "            return\n",
    "\n",
    "        # Local coords\n",
    "        local_x = cuda.threadIdx.x\n",
    "        local_y = cuda.threadIdx.y\n",
    "\n",
    "        # Prepare shared memory\n",
    "        # We store slices from U, V^T, and s in a tile of shape (blockDimX, blockDimY), plus s in 1D\n",
    "        tileU = cuda.shared.array(shape=(blockDimX, blockDimY), dtype=float32)\n",
    "        tileVT= cuda.shared.array(shape=(blockDimX, blockDimY), dtype=float32)\n",
    "        tileS = cuda.shared.array(shape=(blockDimX,),           dtype=float32)\n",
    "        \n",
    "        # Number of chunks\n",
    "        num_chunks = (k + blockDimX - 1) // blockDimX\n",
    "\n",
    "        # Register accumulator\n",
    "        accum = 0.0\n",
    "\n",
    "        for chunk_idx in range(num_chunks):\n",
    "            k_base = chunk_idx * blockDimX\n",
    "\n",
    "            # Load from U => tileU[local_x, local_y]\n",
    "            col_u = k_base + local_y\n",
    "            if i < u.shape[0] and col_u < k:\n",
    "                tileU[local_x, local_y] = u[i, col_u]\n",
    "            else:\n",
    "                tileU[local_x, local_y] = 0.0\n",
    "\n",
    "            # Load from V^T => tileVT[local_x, local_y]\n",
    "            row_vt = k_base + local_x\n",
    "            if row_vt < vt.shape[0] and j < vt.shape[1]:\n",
    "                tileVT[local_x, local_y] = vt[row_vt, j]\n",
    "            else:\n",
    "                tileVT[local_x, local_y] = 0.0\n",
    "\n",
    "            # Load from s => tileS[local_x], only if local_y == 0\n",
    "            if local_y == 0:\n",
    "                if row_vt < s.shape[0]:\n",
    "                    tileS[local_x] = s[row_vt]\n",
    "                else:\n",
    "                    tileS[local_x] = 0.0\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "            # Accumulate partial sums in registers\n",
    "            for n in range(blockDimY):\n",
    "                real_k = k_base + n\n",
    "                if real_k < k:\n",
    "                    accum += tileU[local_x, n] * tileS[local_x] * tileVT[local_x, n]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        C[i, j] = accum\n",
    "\n",
    "    return reconstruct_svd_optimized\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = cuda.stream()\n",
    "\n",
    "results_optimized = []\n",
    "\n",
    "for N in matrix_sizes:\n",
    "    # Get precomputed components\n",
    "    u = svd_components[N]['u']\n",
    "    s = svd_components[N]['s'] \n",
    "    vt = svd_components[N]['vt']\n",
    "    k = svd_components[N]['k']\n",
    "\n",
    "\n",
    "    # Send data to device (on 'stream')\n",
    "    u_dev = cuda.to_device(u, stream=stream)\n",
    "    s_dev = cuda.to_device(s, stream=stream)\n",
    "    vt_dev= cuda.to_device(vt, stream=stream)\n",
    "    C_dev = cuda.device_array((N, N), dtype=np.float32, stream=stream)\n",
    "\n",
    "    stream.synchronize()  # ensure transfers are done\n",
    "\n",
    "    for block_size in block_configs:\n",
    "        blockDimX, blockDimY = block_size\n",
    "        gridX = math.ceil(N / blockDimX)\n",
    "        gridY = math.ceil(N / blockDimY)\n",
    "        grid = (gridX, gridY)\n",
    "\n",
    "        kernel = get_optimized_kernel(block_size)\n",
    "\n",
    "        kernel[grid, block_size, stream](u_dev, s_dev, vt_dev, C_dev, k)\n",
    "        stream.synchronize()\n",
    "\n",
    "        start_gpu = time.time()\n",
    "        kernel[grid, block_size, stream](u_dev, s_dev, vt_dev, C_dev, k)\n",
    "        gpu_time = time.time() - start_gpu\n",
    "\n",
    "        stream.synchronize()\n",
    "\n",
    "        C_gpu = C_dev.copy_to_host(stream=stream)\n",
    "        stream.synchronize()\n",
    "\n",
    "        # Compute speedup\n",
    "        speedup = svd_components[N]['cpu_time'] / gpu_time\n",
    "\n",
    "        results_optimized.append({\n",
    "            'matrix_size': N,\n",
    "            'block_size': block_size,\n",
    "            'cpu_time': svd_components[N]['cpu_time'],\n",
    "            'gpu_time': gpu_time,\n",
    "            'speedup': speedup,\n",
    "        })\n",
    "\n",
    "        print(f\"[N={N}] block={block_size} GPU={gpu_time:.5f}s CPU={cpu_time:.5f}s speedup={speedup:.2f}x\")\n",
    "\n",
    "    # Clean up\n",
    "    del u_dev, s_dev, vt_dev, C_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_optimized, 'Optimized Kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare basic vs shared memory kernel performance\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Get best times for each matrix size\n",
    "best_times_basic = {}\n",
    "best_times_shared = {}\n",
    "best_times_optimized = {}\n",
    "for N in matrix_sizes:\n",
    "    basic_N = [r for r in results_basic if r['matrix_size'] == N]\n",
    "    shared_N = [r for r in results_shared if r['matrix_size'] == N]\n",
    "    optimized_N = [r for r in results_optimized if r['matrix_size'] == N]\n",
    "    best_times_basic[N] = min(basic_N, key=lambda x: x['gpu_time'])\n",
    "    best_times_shared[N] = min(shared_N, key=lambda x: x['gpu_time'])\n",
    "    best_times_optimized[N] = min(optimized_N, key=lambda x: x['gpu_time'])\n",
    "\n",
    "# Plot speedups\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(matrix_sizes, [best_times_basic[N]['speedup'] for N in matrix_sizes], 'b-o', label='Basic Kernel')\n",
    "plt.plot(matrix_sizes, [best_times_shared[N]['speedup'] for N in matrix_sizes], 'r-o', label='Shared Memory Kernel')\n",
    "plt.plot(matrix_sizes, [best_times_optimized[N]['speedup'] for N in matrix_sizes], 'g-o', label='Optimized Kernel')\n",
    "plt.xlabel('Matrix Size')\n",
    "plt.ylabel('Speedup vs CPU')\n",
    "plt.title('Speedup Comparison')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot execution times\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(matrix_sizes, [best_times_basic[N]['gpu_time'] for N in matrix_sizes], 'b-o', label='Basic Kernel')\n",
    "plt.plot(matrix_sizes, [best_times_shared[N]['gpu_time'] for N in matrix_sizes], 'r-o', label='Shared Memory Kernel')\n",
    "plt.plot(matrix_sizes, [best_times_optimized[N]['gpu_time'] for N in matrix_sizes], 'g-o', label='Optimized Kernel')\n",
    "plt.xlabel('Matrix Size')\n",
    "plt.ylabel('Execution Time (s)')\n",
    "plt.title('Execution Time Comparison')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"\\nDetailed Performance Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Matrix Size':^12} | {'Basic Time':^12} | {'Shared Time':^12} | {'Optimized Time':^12} | {'Improvement':^12} | {'Best Block Size':^20}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for N in matrix_sizes:\n",
    "    basic = best_times_basic[N]\n",
    "    shared = best_times_shared[N]\n",
    "    optimized = best_times_optimized[N]\n",
    "    improvement = (basic['gpu_time'] - shared['gpu_time']) / basic['gpu_time'] * 100\n",
    "    \n",
    "    print(f\"{N:^12} | {basic['gpu_time']:^12.4f} | {shared['gpu_time']:^12.4f} | {improvement:^11.1f}% | {shared['block_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 NVIDIA Profiler\n",
    "\n",
    "Benutze einen Performance Profiler von NVIDIA, um Bottlenecks in deinem Code zu identifizieren bzw. unterschiedliche Implementierungen (Blocks, Memory etc.) zu vergleichen. \n",
    "\n",
    "* Siehe Beispiel example_profiling_CUDA.ipynb\n",
    "* [Nsight](https://developer.nvidia.com/nsight-visual-studio-edition) für das Profiling des Codes und die Inspektion der Ergebnisse (neuste Variante)\n",
    "* [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview)\n",
    "* [Nvidia Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual)\n",
    "\n",
    "> Du kannst NVIDIA Nsights Systems und den Nvidia Visual Profiler auf deinem PC installieren und die Leistungsergebnisse aus einer Remote-Instanz visualisieren, auch wenn du keine GPU an/in deinem PC hast. Dafür kannst du die ``*.qdrep`` Datei generieren und danach lokal laden.\n",
    "\n",
    "\n",
    "Dokumentiere deine Analyse ggf. mit 1-2 Visualisierungen und beschreibe, welche Bottlenecks du gefunden bzw. entschärft hast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6 Beschleunigte Rekonstruktion mehrerer Bilder\n",
    "#### 6.1 Implementierung\n",
    "Verwende einige der in bisher gelernten Konzepte, um mehrere Bilder gleichzeitig parallel zu rekonstruieren. Weshalb hast du welche Konzepte für deine Implementierung verwenden? Versuche die GPU konstant auszulasten und so auch die verschiedenen Engines der GPU parallel zu brauchen. Untersuche dies auch für grössere Inputs als die MRI-Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.2 Analyse\n",
    "Vergleiche den Speedup für deine parallele Implementierung im Vergleich zur seriellen Rekonstruktion einzelner Bilder. Analysiere und diskutiere in diesem Zusammenhang die Gesetze von Amdahl und Gustafson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.3 Komponentendiagramm\n",
    "\n",
    "Erstelle das Komponentendiagramm dieser Mini-Challenge für die Rekunstruktion mehrere Bilder mit einer GPU-Implementierung. Erläutere das Komponentendigramm in 3-4 Sätzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild(ern).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7 Reflexion\n",
    "\n",
    "Reflektiere die folgenden Themen indem du in 3-5 Sätzen begründest und anhand von Beispielen erklärst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1: Was sind deiner Meinung nach die 3 wichtigsten Prinzipien bei der Beschleunigung von Code?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2: Welche Rechenarchitekturen der Flynnschen Taxonomie wurden in dieser Mini-Challenge wie verwendet?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3: Haben wir es in dieser Mini-Challenge hauptsächlich mit CPU- oder IO-Bound Problemen zu tun? Nenne Beispiele.\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4: Wie könnte diese Anwendung in einem Producer-Consumer Design konzipiert werden?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5: Was sind die wichtigsten Grundlagen, um mehr Performance auf der GPU in dieser Mini-Challenge zu erreichen?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "6: Reflektiere die Mini-Challenge. Was ist gut gelaufen? Wo gab es Probleme? Wo hast du mehr Zeit als geplant gebraucht? Was hast du dabei gelernt? Was hat dich überrascht? Was hättest du zusätzlich lernen wollen? Würdest du gewisse Fragestellungen anders formulieren? Wenn ja, wie?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
